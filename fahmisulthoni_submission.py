# -*- coding: utf-8 -*-
"""FahmiSulthoni_Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1H1K_EF3AVmlpWp4szocXXW88nNTZVb

*   Nama: Fahmi Sulthoni
*   Email: fahmi.sulthoni30@gmail.com
"""

import tensorflow as tf
print(tf.__version__)

!wget --no-check-certificate \
  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip \
  -O /tmp/rockpaperscissors.zip

import zipfile,os
from sklearn.model_selection import train_test_split
import shutil
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from keras.optimizers import SGD
from matplotlib import pyplot as plt

# Mengekstrak File zip rockpaperscissors.. 
local_zip = '/tmp/rockpaperscissors.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

# Membuat Train Directory dan Validation Directory..
dir_datasetawal = '/tmp/rockpaperscissors'
train_dirc = os.path.join(dir_datasetawal, 'train')
valids_dirc = os.path.join(dir_datasetawal, 'val')

# Membuat direktori terhadap masing masing item (rock, paper, scissor) (Proses Split Data)
dirrocks = os.path.join(dir_datasetawal, 'rock')
dirpapers = os.path.join(dir_datasetawal, 'paper')
dirscissors = os.path.join(dir_datasetawal, 'scissors')

# Set pada train direktory..  (Proses Split Data)
latih_rocks = os.path.join(train_dirc, 'rock')
latih_papers = os.path.join(train_dirc, 'paper')
latih_scissors = os.path.join(train_dirc, 'scissors')

# Set pada validation direktory.. (Proses Split Data)
validrocks = os.path.join(valids_dirc, 'rock')
validpapers = os.path.join(valids_dirc, 'paper')
validscissors = os.path.join(valids_dirc, 'scissors')

# Melakukan Train Test split terhadap data..
# Menetapkan ukuran validasi set menjadi 40% dari dataset..
# Serta memisahkan Data direktory menjadi Data Training dan Data Validation..
latih_rocks_dirc, valid_rocks_dirc = train_test_split(os.listdir(dirrocks), test_size = 0.40)
latih_papers_dirc, valid_papers_dirc = train_test_split(os.listdir(dirpapers), test_size = 0.40)
latih_scissors_dirc, valid_scissors_dirc = train_test_split(os.listdir(dirscissors), test_size = 0.40)

for f in latih_rocks_dirc:
  shutil.copy(os.path.join(dirrocks, f), os.path.join(latih_rocks, f))

for f in latih_papers_dirc:
  shutil.copy(os.path.join(dirpapers, f), os.path.join(latih_papers, f))

for f in latih_scissors_dirc:
  shutil.copy(os.path.join(dirscissors, f), os.path.join(latih_scissors, f))

for f in valid_rocks_dirc:
  shutil.copy(os.path.join(dirrocks, f), os.path.join(validrocks, f))

for f in valid_papers_dirc:
  shutil.copy(os.path.join(dirpapers, f), os.path.join(validpapers, f))

for f in valid_scissors_dirc:
  shutil.copy(os.path.join(dirscissors, f), os.path.join(validscissors, f))

trains_datagents = ImageDataGenerator (
    rescale = 1./255,
    rotation_range = 20,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest'
)
testing_datagents = ImageDataGenerator (
    rescale = 1./255,
    rotation_range = 20,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest'
)

trains_generats = trains_datagents.flow_from_directory (
    train_dirc,
    target_size=(150,150),
    batch_size = 32,
    class_mode = 'categorical'
)
valid_generats = testing_datagents.flow_from_directory (
    valids_dirc,
    target_size=(150,150),
    batch_size = 32,
    class_mode = 'categorical'
)

s_models = tf.keras.models.Sequential([
      tf.keras.layers.Conv2D(32, (3,3), activation= 'relu', input_shape = (150,150,3)),
      tf.keras.layers.MaxPool2D(2,2),
      tf.keras.layers.Conv2D(64, (3,3), activation= 'relu'),
      tf.keras.layers.MaxPool2D(2,2),
      tf.keras.layers.Conv2D(128, (3,3), activation= 'relu'),
      tf.keras.layers.MaxPool2D(2,2),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(512,activation= 'relu'),
      tf.keras.layers.Dense(3, activation= 'softmax')
])

s_models.compile(
    loss= 'categorical_crossentropy',
    optimizer = tf.optimizers.Adam(),
    metrics = ['accuracy']
)

s_models.summary()

optimal = SGD(lr=0.01, momentum=0.9)
s_models.compile(loss = 'categorical_crossentropy', optimizer=optimal, metrics=['accuracy'])

# Menggunakan Callback untuk menghindari overfitting..
callbacks = tf.keras.callbacks.ModelCheckpoint(filepath='model.h5', verbose=1, save_best_only=True)

history = s_models.fit (
    trains_generats,
    steps_per_epoch = 41,
    epochs = 20,
    validation_data = valid_generats,
    validation_steps = 27,
    verbose = 2,
    callbacks = [callbacks]
)

# Grafik untuk Loss Train dan Loss Validation..

plt.plot(history.history['loss'], label ='Training loss')
plt.plot(history.history['val_loss'], label = 'Validation loss')
plt.title('Loss Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Grafik untuk Akurasi Train dan Akurasi validation..
plt.plot(history.history['accuracy'], label = 'Train Accuracy')
plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')
plt.title('Accuracy Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

uploaded = files.upload()

for file_myimage in uploaded.keys():

  path = file_myimage
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images= np.vstack([x])
  classes = s_models.predict(images, batch_size=10)

  print(file_myimage)
  if classes[0,0] !=0:
    print('kertas')
  elif classes[0,1] !=0:
    print('batu')
  else:
    print('gunting')